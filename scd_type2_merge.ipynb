{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e84ac5-d3a8-4132-9930-53fd8565aacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import os\n",
    "\n",
    "print(os.environ['SPARK_VERSION'])\n",
    "# Get job parameters from Databricks\n",
    "date_str = dbutils.widgets.get(\"arrival_date\")\n",
    "# date_str = \"2024-07-25\"\n",
    "\n",
    "booking_data = f\"/Volumes/incremental_load/default/orders_data/booking_data/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/incremental_load/default/orders_data/customer_data/customers_{date_str}.csv\"\n",
    "\n",
    "print(booking_data)\n",
    "print(customer_data)\n",
    "\n",
    "booking_df = spark.read.format(\"csv\").option(\"header\", \"True\").option(\"inferSchema\", \"True\").option(\"quote\",\"\\\"\").option(\"multiLine\",\"True\").load(booking_data)\n",
    "booking_df.printSchema()\n",
    "display(booking_df)\n",
    "\n",
    "customer_df = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\").option(\"quote\",\"\\\"\").option(\"multiLine\",\"True\").load(customer_data)\n",
    "customer_df.printSchema()\n",
    "display(customer_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ab705b-b04a-4240-9df1-55c8c08843dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Quality Checks on booking data\n",
    "check_incremental = Check(spark, CheckLevel.Error, \"Booking Data Check\") \\\n",
    "    .hasSize(lambda x: x > 0) \\\n",
    "    .isUnique(\"booking_id\", hint=\"Booking ID is not unique throught\") \\\n",
    "    .isComplete(\"customer_id\") \\\n",
    "    .isComplete(\"amount\") \\\n",
    "    .isNonNegative(\"amount\") \\\n",
    "    .isNonNegative(\"quantity\") \\\n",
    "    .isNonNegative(\"discount\")\n",
    "\n",
    "# Data Quality Checks on customer data\n",
    "# check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
    "#     .hasSize(lambda x: x > 0) \\\n",
    "#     .isUnique(\"customer_id\") \\\n",
    "#     .isComplete(\"customer_name\") \\\n",
    "#     .isComplete(\"customer_address\") \\\n",
    "#     .isComplete(\"phone_number\") \\\n",
    "#     .isComplete(\"email\")\n",
    "\n",
    "check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
    "    .hasSize(lambda x: x > 0) \\\n",
    "    .isUnique(\"customer_id\") \\\n",
    "    .isComplete(\"customer_name\") \\\n",
    "    .isComplete(\"customer_address\") \\\n",
    "    .isComplete(\"email\")\n",
    "\n",
    "# Run the verification suite\n",
    "booking_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(booking_df) \\\n",
    "    .addCheck(check_incremental) \\\n",
    "    .run()\n",
    "\n",
    "customer_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(customer_df) \\\n",
    "    .addCheck(check_scd) \\\n",
    "    .run()\n",
    "\n",
    "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
    "display(booking_dq_check_df)\n",
    "\n",
    "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, customer_dq_check)\n",
    "display(customer_dq_check_df)\n",
    "\n",
    "# Check if verification passed\n",
    "if booking_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Booking Data\")\n",
    "\n",
    "if customer_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Customer Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc81288-7c8c-422b-bdfb-e34200d1df26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add ingestion timestamp to booking data\n",
    "booking_df_incremental = booking_df.withColumn(\"ingestion_time\", current_timestamp())\n",
    "display(booking_df_incremental)\n",
    "\n",
    "# Join booking data with customer data\n",
    "df_joined = booking_df_incremental.join(customer_df, \"customer_id\")\n",
    "display(df_joined)\n",
    "\n",
    "# Business transformation: calculate total cost after discount and filter\n",
    "df_transformed = df_joined \\\n",
    "    .withColumn(\"total_cost\", col(\"amount\") - col(\"discount\")) \\\n",
    "    .filter(col(\"quantity\") > 0)\n",
    "display(df_transformed)\n",
    "\n",
    "# Group by and aggregate df_transformed\n",
    "df_transformed_agg = df_transformed \\\n",
    "    .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_cost\").alias(\"total_amount_sum\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity_sum\")\n",
    "    )\n",
    "display(df_transformed_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b76aff5-d0c7-4c87-8bf5-0a65ee50fcb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the Delta table exists\n",
    "fact_table_path = \"incremental_load.default.booking_fact\"\n",
    "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table_path)\n",
    "\n",
    "\n",
    "if fact_table_exists:\n",
    "    # Read the existing fact table\n",
    "    df_existing_fact = spark.read.format(\"delta\").table(fact_table_path)\n",
    "    display(df_existing_fact)\n",
    "    \n",
    "    # Combine the aggregated data\n",
    "    df_combined = df_existing_fact.unionByName(df_transformed_agg, allowMissingColumns=True)\n",
    "    display(df_combined)\n",
    "    \n",
    "    # Perform another group by and aggregation on the combined data\n",
    "    df_final_agg = df_combined \\\n",
    "        .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "        .agg(\n",
    "            _sum(\"total_amount_sum\").alias(\"total_amount_sum\"),\n",
    "            _sum(\"total_quantity_sum\").alias(\"total_quantity_sum\")\n",
    "        )\n",
    "    display(df_final_agg)\n",
    "else:\n",
    "    # If the fact table doesn't exist, use the aggregated transformed data directly\n",
    "    df_final_agg = df_transformed_agg\n",
    "    print(\"Fact table doesn't exist so used the aggregated transformed data directly\")\n",
    "\n",
    "display(df_final_agg)\n",
    "\n",
    "# Write the final aggregated data back to the Delta table\n",
    "df_final_agg.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(fact_table_path)\n",
    "display(df_final_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e087e32-500c-449f-a1ad-cb2c20575f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd_table_path = \"incremental_load.default.customer_dim\"\n",
    "scd_table_exists = spark._jsparkSession.catalog().tableExists(scd_table_path)\n",
    "\n",
    "# Check if the customers table exists\n",
    "if scd_table_exists:\n",
    "    # Load the existing SCD table\n",
    "    scd_table = DeltaTable.forName(spark, scd_table_path)\n",
    "    display(scd_table.toDF())\n",
    "    \n",
    "    # Perform SCD2 merge logic\n",
    "    scd_table.alias(\"scd\") \\\n",
    "        .merge(\n",
    "            customer_df.alias(\"updates\"),\n",
    "            \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"valid_to\": \"updates.valid_from\",\n",
    "        }) \\\n",
    "        .execute()\n",
    "    display(scd_table.toDF())\n",
    "\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_table_path)\n",
    "else:\n",
    "    # If the SCD table doesn't exist, write the customer data as a new Delta table\n",
    "    print(\"SCD table doesn't exist so writing the customer data as a new Delta table\")\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(scd_table_path)\n",
    "    display(customer_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scd_type2_merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
